import json, os, re
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# =====================
# PARAMETERS
# =====================
MODEL_PATH = "deepseek-ai/deepseek-coder-1.3b-instruct"

PROMPTS = {
    "CoT": """You are a helpful Python programmer.
Solve the following problem step-by-step and then output the final implementation.

Problem:
{problem}

Please:
1. Restate the problem briefly.
2. Write a short reasoning or plan.
3. Then give only the function code in a Python code block.""",

    "SelfDebug": """You are a careful programmer who writes and fixes code.

Problem:
{problem}

Steps:
1. Write the Python solution.
2. Then test it on a few examples (as text, not execution).
3. If you find any issue, rewrite the corrected code below.

End your answer with the final correct function only."""
}

MAX_NEW_TOKENS = 512       
TEMPERATURE = 0.2       

# =====================
# UTILITIES
# =====================
def extract_code(text: str) -> str:
    """Extract the last Python code block, fix unterminated docstrings"""
    code_blocks = re.findall(r"```(?:python)?\n(.*?)```", text, re.DOTALL | re.IGNORECASE)
    if code_blocks:
        code = code_blocks[-1].strip()
        # Fix unterminated triple quotes at the end
        if code.count('"""') % 2 != 0:
            code += '"""'
        return code
    # fallback: take from 'def' keyword to end
    m = re.search(r"(def\s+[\s\S]+)", text)
    return m.group(1).strip() if m else text.strip()

def make_pipeline(model_path):
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    kwargs = {"device_map": "auto"}
    if torch.cuda.is_available():
        kwargs["torch_dtype"] = torch.float16
    model = AutoModelForCausalLM.from_pretrained(model_path, **kwargs)
    pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
    return pipe

# =====================
# MAIN
# =====================
def main():
    # Load problems
    with open("new_data.json") as f:
        problems = json.load(f)

    # Only use the first 10 problems
    problems = problems[:10]

    # Initialize pipeline
    print(f"Loading model {MODEL_PATH} ...")
    pipe = make_pipeline(MODEL_PATH)

    # Generate code
    for strategy, template in PROMPTS.items():
        outdir = f"outputs/deepseek_{strategy}"
        os.makedirs(outdir, exist_ok=True)
        print(f"\n=== Generating for DeepSeek | {strategy} -> {outdir} ===")
        for idx, prob in enumerate(tqdm(problems)):
            prompt = template.format(problem=prob["prompt"])
            try:
                res = pipe(
                    prompt,
                    max_new_tokens=MAX_NEW_TOKENS,
                    temperature=TEMPERATURE,
                    do_sample=False,
                    num_return_sequences=1
                )[0]["generated_text"]
            except Exception as e:
                print(f"Generation error for {prob.get('id','?')}: {e}")
                res = ""

            code = extract_code(res)
            
            # Check syntax
            try:
                compile(code, "<string>", "exec")
            except SyntaxError as e:
                print(f"Syntax error in {prob.get('id','?')}: {e}")
                code = "# Failed to generate valid Python code"

            safe_id = prob.get("id", prob.get("task_id", f"prob_{idx}"))
            fname = os.path.join(outdir, f"{safe_id}.py")
            with open(fname, "w") as f:
                f.write(f"# Generated by model: DeepSeek\n# Strategy: {strategy}\n\n")
                f.write(code)

    print("\nFull generation finished.")

if __name__ == "__main__":
    main()

