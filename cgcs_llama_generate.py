import json, os, re
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# =====================
# PARAMETERS
# =====================
MODEL_PATH = "Stable-Code-Instruct-3B"  # path to the local model folder

PROMPTS = {
    "CGCS": """You are an expert Python developer. Solve the problem using a Constraint-Guided Code Synthesis approach.

Problem:
{problem}

Instructions:
1. List all constraints and edge cases that must be handled.
2. For each code block you write, tag it with the constraints it satisfies.
3. After each block, reason through an example to verify correctness.
4. Only produce final code after all constraints are verified.
5. Provide clean, commented Python code at the end that meets all constraints.

Example format:
# Constraint: {describe constraint}
# Code block
{code}
# Reasoning: {verify with example}
"""
}

MAX_NEW_TOKENS = 512      # token limit for generation
TEMPERATURE = 0.2          # greedy decoding

# =====================
# UTILITIES
# =====================
def extract_code(text: str) -> str:
    """Extract the last Python code block from the model output."""
    code_blocks = re.findall(r"```(?:python)?\n(.*?)```", text, re.DOTALL | re.IGNORECASE)
    if code_blocks:
        return code_blocks[-1].strip()
    m = re.search(r"(def\s+[\s\S]+)", text)
    return m.group(1).strip() if m else text.strip()

def make_pipeline(model_path):
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        torch_dtype=torch.float16 if torch.has_mps else torch.float32
    )
    device = model.device
    return tokenizer, model, device

# =====================
# MAIN
# =====================
def main():
    # Load problems
    with open("humaneval_subset.json") as f:
        problems = json.load(f)

    # Only use first 10 problems
    problems = problems[:10]

    # Load model & tokenizer
    print(f"Loading model {MODEL_PATH} ...")
    tokenizer, model, device = make_pipeline(MODEL_PATH)
    print(f"Model loaded on {device}")

    # Generate code using CGCS
    strategy = "CGCS"
    template = PROMPTS[strategy]
    outdir = f"outputs/llama3_{strategy}"
    os.makedirs(outdir, exist_ok=True)
    print(f"\n=== Generating for strategy {strategy} -> {outdir} ===")

    for idx, prob in enumerate(tqdm(problems)):
        prompt = template.format(problem=prob["prompt"])
        try:
            inputs = tokenizer(prompt, return_tensors="pt").to(device)
            outputs = model.generate(
                inputs["input_ids"],
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=False  # greedy decoding
            )
            res = tokenizer.decode(outputs[0], skip_special_tokens=True)
        except Exception as e:
            print(f"Generation error for {prob.get('id','?')}: {e}")
            res = ""

        code = extract_code(res)
        safe_id = prob.get("id", f"prob_{idx}")
        fname = os.path.join(outdir, f"{safe_id}.py")
        with open(fname, "w") as f:
            f.write(f"# Generated by model: Stable-Code-Instruct-3B\n# Strategy: {strategy}\n\n")
            f.write(code)

    print("\nGeneration finished for all 10 problems.")

if __name__ == "__main__":
    main()
