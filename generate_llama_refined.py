import json, os, re
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# =====================
# PARAMETERS
# =====================
MODEL_PATH = "Stable-Code-Instruct-3B"  # path to the local model folder
PROMPTS = {
    "CoT": """Generate the problem using Chain of Thought.
Solve the following problem step-by-step and then output the final implementation.

Problem:
{problem}

Please:
1. Restate the problem briefly.
2. Write a short reasoning or plan.
3. Then give only the function code in a Python code block.""",

    "SelfDebug": """Generate the problem using Self Debugging.

Problem:
{problem}

Steps:
1. Write the Python solution.
2. Then test it on a few examples (as text, not execution).
3. If you find any issue, rewrite the corrected code below.

End your answer with the final correct function only."""
}

MAX_NEW_TOKENS = 512    
TEMPERATURE = 0.2         

# =====================
# UTILITIES
# =====================
def extract_code(text: str) -> str:
    code_blocks = re.findall(r"```(?:python)?\n(.*?)```", text, re.DOTALL | re.IGNORECASE)
    if code_blocks:
        return code_blocks[-1].strip()
    m = re.search(r"(def\s+[\s\S]+)", text)
    return m.group(1).strip() if m else text.strip()

# =====================
# MAIN
# =====================
def main():
    # Load problems
    with open("refined_prompt_dataset.json") as f:
        problems = json.load(f)

    # Only use first 10 problems
    problems = problems[:10]

    # Load model & tokenizer
    print(f"Loading model {MODEL_PATH} ...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        device_map="auto",        # auto assigns to CPU/MPS/GPU
        torch_dtype=torch.float16 if torch.has_mps else torch.float32
    )

    device = model.device
    print(f"Model loaded on {device}")

    # Generate code
    for strategy, template in PROMPTS.items():
        outdir = f"outputs/ llama3_{strategy}"
        os.makedirs(outdir, exist_ok=True)
        print(f"\n=== Generating for strategy {strategy} -> {outdir} ===")
        for idx, prob in enumerate(tqdm(problems)):
            prompt = template.format(problem=prob["prompt"])
            try:
                inputs = tokenizer(prompt, return_tensors="pt").to(device)
                outputs = model.generate(
                    inputs["input_ids"],
                    max_new_tokens=MAX_NEW_TOKENS,
                    do_sample=False,        # greedy decoding
                )
                res = tokenizer.decode(outputs[0], skip_special_tokens=True)
            except Exception as e:
                print(f"Generation error for {prob.get('id','?')}: {e}")
                res = ""

            code = extract_code(res)
            safe_id = prob.get("id", f"prob_{idx}")
            fname = os.path.join(outdir, f"{safe_id}.py")
            with open(fname, "w") as f:
                f.write(f"# Generated by model: Stable-Code-Instruct-3B\n# Strategy: {strategy}\n\n")
                f.write(code)

    print("\nGeneration finished for all 10 problems.")

if __name__ == "__main__":
    main()

